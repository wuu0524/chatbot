from flask import Flask, request, jsonify
import os
from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory
import traceback

app = Flask(__name__)

# GPT åˆå§‹åŒ–ï¼ˆç”¨ LangChain åŒ…è£ï¼‰
llm = ChatOpenAI(
    temperature=0,
    model_name="gpt-3.5-turbo",
    openai_api_key=os.getenv("OPENAI_API_KEY")
)

# å°è©±è¨˜æ†¶æ©Ÿåˆ¶
memory = ConversationBufferMemory(return_messages=True)
conversation = ConversationChain(llm=llm, memory=memory, verbose=False)

# Dialogflow webhook endpoint
@app.route('/webhook', methods=['POST'])
def webhook():
    req = request.get_json()
    user_input = req['queryResult']['queryText']

    try:
        reply = conversation.predict(input=user_input)
        print("ğŸ§  GPT å›æ‡‰ï¼š", reply)
        return jsonify({"fulfillmentText": reply})
    except Exception as e:
        print("âŒ éŒ¯èª¤ï¼š", str(e))
        traceback.print_exc()  # â† å°å‡ºå®Œæ•´éŒ¯èª¤å †ç–Š
        return jsonify({"fulfillmentText": f"ç³»çµ±éŒ¯èª¤ï¼š{str(e)}"})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=10000)
