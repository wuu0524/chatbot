from flask import Flask, request, jsonify
import os
from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory

app = Flask(__name__)

# GPT åˆå§‹åŒ–ï¼ˆç”¨ LangChain åŒ…è£ï¼‰
llm = ChatOpenAI(
    temperature=0,
    model_name="gpt-3.5-turbo",
openai_api_key = os.getenv("OPENAI_API_KEY")

# å°è©±è¨˜æ†¶æ©Ÿåˆ¶ï¼šè¨˜ä½éå»å°è©±
memory = ConversationBufferMemory(return_messages=True)
conversation = ConversationChain(llm=llm, memory=memory, verbose=False)

# Dialogflow çš„ webhook endpoint
@app.route('/webhook', methods=['POST'])
def webhook():
    req = request.get_json()
    user_input = req['queryResult']['queryText']

    try:
        # ç”¨ GPT å›æ‡‰ï¼Œä¸¦å¸¶å…¥è¨˜æ†¶
        reply = conversation.predict(input=user_input)
        print("ğŸ§  GPT å›æ‡‰ï¼š", reply)
        return jsonify({"fulfillmentText": reply})
    except Exception as e:
        print("âŒ éŒ¯èª¤ï¼š", str(e))
        return jsonify({"fulfillmentText": "ç³»çµ±ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=10000)
